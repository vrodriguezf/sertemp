{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Forecasting\n",
    "Forecasting is a technique that uses historical data as inputs to make informed estimates that are predictive in determining the direction of future trends. It's a planning tool that helps management in its attempts to cope with the uncertainty of the future, relying mainly on data from the past and present and analysis of trends. Forecasting is used in various areas such as weather prediction, sales projection, stock market trends, and many more. In the context of time series data, forecasting is often associated with making predictions about the future based on time-dependent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the parameters\n",
    "n_samples = 1000  # Number of samples in the time series\n",
    "seasonality_period = 12  # Period of the cosine seasonality\n",
    "noise_std = 0.2  # Standard deviation of the noise\n",
    "\n",
    "# Generate the time index\n",
    "t = np.arange(n_samples)\n",
    "\n",
    "# Generate the cosine seasonality component\n",
    "seasonality = np.cos(2 * np.pi * t / seasonality_period)\n",
    "\n",
    "# Generate the noise component\n",
    "noise = np.random.normal(0, noise_std, n_samples)\n",
    "\n",
    "# Combine the components to create the synthetic time series\n",
    "time_series = seasonality + noise\n",
    "\n",
    "# Plot the synthetic time series\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(time_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train-test split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Calculate the index to split the time series\n",
    "split_index = int(len(time_series) * train_ratio)\n",
    "\n",
    "# Split the time series into train and test sets\n",
    "train_set = time_series[:split_index]\n",
    "test_set = time_series[split_index:]\n",
    "\n",
    "# Print the train and test sets\n",
    "print(\"Train set:\", train_set[:10])\n",
    "print(\"Test set:\", test_set[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_set, columns=[\"value\"])\n",
    "test_df = pd.DataFrame(test_set, columns=[\"value\"])\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_validate(train, test, predictions):\n",
    "    if len(test.columns) > 1:\n",
    "        for i, column in enumerate(test.columns):\n",
    "            rmse = np.sqrt(mean_squared_error(test[column], predictions[:, i]))\n",
    "            print(f\"Test RMSE for {column}: {rmse:.3f}\")\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(test.values, predictions))\n",
    "        print(f\"Test RMSE: {rmse:.3f}\")\n",
    "        \n",
    "    plt.figure(figsize=(18,6))\n",
    "    # Plot train set\n",
    "    plt.plot(train, label='Train Set')\n",
    "\n",
    "    # Plot test set\n",
    "    test_x = np.arange(len(train), len(train)+len(test))\n",
    "    plt.plot(test_x, test, label='Test Set')\n",
    "    plt.plot(test_x, predictions, label='Predictions')\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Predictions vs Ground Truth')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = [train_df.values[-1]]*len(test_set)\n",
    "rmse = np.sqrt(mean_squared_error(test_df.values, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot train set\n",
    "plot_and_validate(train_df, test_df, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Moving Average Demonstration\n",
    "Demonstrate the use of Simple Moving Average in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional required library\n",
    "import numpy as np\n",
    "\n",
    "# Simple Moving Average model\n",
    "def model_sma(history, window_size):\n",
    "    return np.mean(history[-window_size:])\n",
    "\n",
    "# Walk-forward validation for SMA\n",
    "predictions_sma = list()\n",
    "history = train_df['value'].values.tolist()\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    # Predict and append to predictions\n",
    "    yhat = model_sma(history, 24)\n",
    "    predictions_sma.append(yhat)\n",
    "    \n",
    "    # Add actual observation to history for the next loop\n",
    "    history.append(yhat)\n",
    "\n",
    "plot_and_validate(train_df, test_df, predictions_sma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Smoothing Demonstration\n",
    "Demonstrate the use of Exponential Smoothing in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from numpy import sqrt\n",
    "\n",
    "# Fit the model\n",
    "model = SimpleExpSmoothing(train_df['value'])\n",
    "model_fit = model.fit(smoothing_level=0.1, optimized=False)\n",
    "\n",
    "# Make prediction\n",
    "yhat = model_fit.forecast(len(test_df))\n",
    "\n",
    "# Walk-forward validation for Exponential Smoothing\n",
    "predictions_exp = list()\n",
    "history = train_df['value'].values.tolist()\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    # Predict and append to predictions\n",
    "    yhat = model_fit.predict(len(history), len(history))\n",
    "    predictions_exp.append(yhat)\n",
    "    \n",
    "    # Add actual observation to history for the next loop\n",
    "    history.append(test_df['value'].values[i])\n",
    "\n",
    "plot_and_validate(train_df, test_df, predictions_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA and SARIMA Demonstration\n",
    "Demonstrate the use of ARIMA and SARIMA models in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# ARIMA model\n",
    "model_arima = ARIMA(train_df['value'], order=(3,2,10))\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Make predictions\n",
    "predictions_arima = model_arima_fit.predict(start=len(train_df), end=len(train_df) + len(test_df) - 1)\n",
    "plot_and_validate(train_df, test_df, predictions_arima)\n",
    "\n",
    "# SARIMA model\n",
    "model_sarima = SARIMAX(train_df['value'], order=(1,0,1), seasonal_order=(1, 1, 0, 12))\n",
    "model_sarima_fit = model_sarima.fit()\n",
    "\n",
    "# Make predictions\n",
    "predictions_sarima = model_sarima_fit.predict(start=len(train_df), end=len(train_df) + len(test_df) - 1)\n",
    "plot_and_validate(train_df, test_df, predictions_sarima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet Demonstration\n",
    "Demonstrate the use of Prophet in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library\n",
    "from prophet import Prophet\n",
    "\n",
    "# Initialize the Model\n",
    "model_prophet = Prophet(interval_width=0.95)\n",
    "new_df = train_df.reset_index().rename(columns={'index':'ds', 'value':'y'})\n",
    "new_df['ds'] = pd.to_datetime(new_df['ds'], unit='M')\n",
    "\n",
    "# Fit the Model\n",
    "model_prophet.fit(new_df)\n",
    "\n",
    "# Make Future Dataframe\n",
    "future = model_prophet.make_future_dataframe(periods=len(test_df), freq='M', include_history=False)\n",
    "\n",
    "# Make Predictions\n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "# Extract Predicted Values\n",
    "predictions_prophet = forecast['yhat'].values[-len(test_df):]\n",
    "\n",
    "# Calculate RMSE for Prophet\n",
    "rmse_prophet = sqrt(mean_squared_error(test_df, predictions_prophet))\n",
    "plot_and_validate(train_df, test_df, predictions_prophet)\n",
    "\n",
    "# Use the plot with confidence intervals\n",
    "model_prophet.plot(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Autoregression Demonstration\n",
    "VAR is used for multivariate time series, we define a second new variable to perform forecasting on the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the parameters\n",
    "trend_coeff = 5e-6\n",
    "seasonality_period = 24\n",
    "noise_std = 0.1\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate the time index\n",
    "t = np.arange(n_samples)\n",
    "\n",
    "# Generate the quadratic ascending trend\n",
    "trend = trend_coeff * t**2\n",
    "\n",
    "# Generate the sine seasonality\n",
    "seasonality = np.sin(2* np.pi * t / seasonality_period)\n",
    "\n",
    "# Generate the random noise\n",
    "noise = np.random.normal(0, noise_std, n_samples)\n",
    "\n",
    "# Generate the synthetic time series\n",
    "time_series = trend + seasonality + noise\n",
    "\n",
    "# Assuming your synthetic time series is stored in the variable 'data'\n",
    "plt.plot(time_series)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Synthetic Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = time_series[:split_index]\n",
    "test_set = time_series[split_index:]\n",
    "\n",
    "train_df['aux_value'] = train_set\n",
    "test_df['aux_value'] = test_set\n",
    "\n",
    "# Plot the synthetic multi-variate time series\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(train_df['value'], label='Main variable')\n",
    "plt.plot(train_df['aux_value'], label='Secondary variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Normalize features to a range\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df.values)\n",
    "test_scaled = scaler.transform(test_df.values)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=train_df.columns)\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=test_df.columns)\n",
    "\n",
    "# Fit the model\n",
    "model = VAR(endog=train_scaled)\n",
    "model_fit = model.fit(maxlags=12) # maxlags is the maximum number of lags to consider, use 24 to fix the issue\n",
    "\n",
    "# Make prediction\n",
    "prediction = model_fit.forecast(model_fit.endog, steps=len(test_df))\n",
    "\n",
    "# Invert transformation\n",
    "prediction = scaler.inverse_transform(prediction)\n",
    "plot_and_validate(train_df, test_df, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Demonstration\n",
    "Any ML regressor can be used to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the model\n",
    "model_rf = RandomForestRegressor(n_estimators=500, random_state=0)\n",
    "\n",
    "sliding_window = 48\n",
    "# Prepare train data\n",
    "slide_train_df = pd.DataFrame()\n",
    "for i in range(sliding_window+1):\n",
    "    for column in train_df.columns:\n",
    "        slide_train_df[f'{column}-{i}'] = train_scaled[column].shift(i)\n",
    "\n",
    "slide_train_df.dropna(inplace=True)\n",
    "slide_train_df_shuffled = slide_train_df.sample(frac=1, random_state=0)\n",
    "slide_train_df_x = slide_train_df_shuffled.iloc[:,2:]\n",
    "slide_train_df_y = slide_train_df_shuffled.iloc[:,:2]\n",
    "\n",
    "# Fit the model\n",
    "model_rf.fit(slide_train_df_x.values, slide_train_df_y.values)\n",
    "\n",
    "# Generate predictions\n",
    "test_inputs = slide_train_df.iloc[-1, 2:].values.reshape(1, -1)\n",
    "prediction_rf = []\n",
    "for i in range(len(test_df)):\n",
    "    prediction = model_rf.predict(test_inputs)\n",
    "    prediction_rf.append(prediction)\n",
    "    test_inputs = np.roll(test_inputs, 2)\n",
    "    test_inputs[0, :2] = prediction\n",
    "\n",
    "prediction_rf = scaler.inverse_transform(np.array(prediction_rf).reshape(-1, 2))\n",
    "plot_and_validate(train_df, test_df, prediction_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Demonstration\n",
    "Demonstrate the use of LSTM model (with PyTorch) in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.reset_hidden_cell()\n",
    "\n",
    "    def reset_hidden_cell(self):\n",
    "        self.hidden_cell = (torch.zeros(1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,self.hidden_layer_size))\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
    "        return self.linear(lstm_out)\n",
    "\n",
    "# Prepare data for LSTM\n",
    "train_data_normalized = torch.FloatTensor(train_scaled.values).view(-1, 2)\n",
    "\n",
    "# Define a method to create in-out sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw-1]\n",
    "        train_label = input_data[i+1:i+tw]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "\n",
    "# Create in-out sequences\n",
    "window_size = 12\n",
    "train_inout_seq = create_inout_sequences(train_data_normalized, window_size)\n",
    "\n",
    "# Initialize the model, define loss and optimization functions\n",
    "model = LSTM(input_size=2, hidden_layer_size=50, output_size=2)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    rmse = 0\n",
    "    random.shuffle(train_inout_seq) # Randomize the training data\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.reset_hidden_cell()\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred.squeeze(), labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_loss += single_loss/len(train_inout_seq)\n",
    "            rmse += np.sqrt(mean_squared_error(scaler.inverse_transform(labels.detach().numpy()), \n",
    "                                              scaler.inverse_transform(y_pred.squeeze().detach().numpy()))\n",
    "                         )/len(train_inout_seq)\n",
    "\n",
    "    print(f'Epoch {i} loss: {epoch_loss}. RMSE: {rmse}', end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "fut_pred = len(test_scaled)\n",
    "test_inputs = train_data_normalized[-window_size:].tolist()\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model.reset_hidden_cell()\n",
    "    seq = torch.FloatTensor(test_inputs[-window_size:])\n",
    "    output = model(seq)[-1].unsqueeze(0)\n",
    "    test_inputs.append(output.numpy())\n",
    "\n",
    "    for i in tqdm(range(fut_pred)):\n",
    "        output = model(output)\n",
    "        test_inputs.append(output[-1].numpy())\n",
    "\n",
    "# Invert transformations\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[-fut_pred:]).reshape(-1, 2))\n",
    "\n",
    "# Calculate RMSE for LSTM\n",
    "plot_and_validate(train_df, test_df, actual_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
