{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate quantiles and long horizon forecasting with N-HiTS\n",
    "This nb was taken from [the PyTorch Forecasting documentation.](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/nhits.html)\n",
    "\n",
    "We recommend running it here [https://colab.research.google.com/github/vrodriguezf/sertemp/blob/main/nbs/forecasting/NHiTS.ipynb](https://colab.research.google.com/github/vrodriguezf/sertemp/blob/main/nbs/forecasting/NHiTS.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies:\n",
    "!pip install lightning pytorch_forecasting cpflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies --> lightning and pytorch-forecasting\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, NHiTS, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, MQF2DistributionLoss, QuantileLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a synthetic dataset to demonstrate the network's capabilities. The data consists of a quadratic trend and a seasonality component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_ar_data(seasonality=10.0, timesteps=400, n_series=100, seed=42)\n",
    "data[\"static\"] = 2\n",
    "data[\"date\"] = pd.Timestamp(\"2020-01-01\") + pd.to_timedelta(data.time_idx, \"D\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Before starting training, we need to split the dataset into a training and validation :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 20\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "context_length = max_encoder_length\n",
    "prediction_length = max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    categorical_encoders={\"series\": NaNLabelEncoder().fit(data.series)},\n",
    "    group_ids=[\"series\"],\n",
    "    # only unknown variable is \"value\" - and N-HiTS can also not take any additional variables\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    max_encoder_length=context_length,\n",
    "    max_prediction_length=prediction_length,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training_cutoff + 1)\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate baseline error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Our baseline model predicts future values by repeating the last know value. The resulting :py:class:`~pytorch_forecasting.metrics.SMAPE` is disappointing and should not be easy to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate baseline absolute error\n",
    "baseline_predictions = Baseline().predict(val_dataloader, trainer_kwargs=dict(accelerator=\"cpu\"), return_y=True)\n",
    "SMAPE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Finding the optimal learning rate using [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/) is easy. The key hyperparameter of the :py:class:`~pytorch_forecasting.models.nhits.NHiTS` model is ``hidden_size``.\n",
    "\n",
    "PyTorch Forecasting is flexible enough to use NHiTS with different loss functions, enabling not only point forecasts but also probabilistic ones. Here, we will demonstrate not only a typical quantile regression but a multivariate quantile regression with :py:class:`~pytorch_forecasting.metrics.distributions.MQF2DistributionLoss` which allows calculation sampling consistent paths along a prediction horizon. This allows calculation of quantiles, e.g. of the sum over the prediction horizon while still avoiding the accumulating error problem from auto-regressive methods such as DeepAR. One needs to install an additional package for this quantile function:\n",
    "\n",
    "``pip install pytorch-forecasting[mqf2]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(accelerator=\"cpu\", gradient_clip_val=0.1)\n",
    "net = NHiTS.from_dataset(\n",
    "    training,\n",
    "    learning_rate=3e-2,\n",
    "    weight_decay=1e-2,\n",
    "    loss=MQF2DistributionLoss(prediction_length=max_prediction_length),\n",
    "    backcast_loss_ratio=0.0,\n",
    "    hidden_size=64,\n",
    "    optimizer=\"AdamW\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    net, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, min_lr=1e-5, max_lr=1e-1\n",
    ")\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "net.hparams.learning_rate = res.suggestion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    callbacks=[early_stop_callback],\n",
    "    limit_train_batches=30,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "net = NHiTS.from_dataset(\n",
    "    training,\n",
    "    learning_rate=5e-3,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    weight_decay=1e-2,\n",
    "    backcast_loss_ratio=0.0,\n",
    "    hidden_size=64,\n",
    "    optimizer=\"AdamW\",\n",
    "    loss=MQF2DistributionLoss(prediction_length=max_prediction_length),\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = NHiTS.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We predict on the validation dataset with :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.predict` and calculate the error which is well below the baseline error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(val_dataloader, trainer_kwargs=dict(accelerator=\"cpu\"), return_y=True)\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at random samples from the validation set is always a good way to understand if the forecast is reasonable - and it is!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = best_model.predict(val_dataloader, mode=\"raw\", return_x=True, trainer_kwargs=dict(accelerator=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_model.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can ask PyTorch Forecasting to decompose the prediction into blocks which focus on a different frequency spectrum, e.g. seasonality and trend with :py:meth:`~pytorch_forecasting.models.nhits.NHiTS.plot_interpretation`. This is a special feature of the :py:class:`~pytorch_forecasting.models.nhits.NHiTS` model and only possible because of its unique architecture. The results show that there seem to be many ways to explain the data and the algorithm does not always chooses the one making intuitive sense. This is partially down to the small number of time series we trained on (100). But it is also due because our forecasting period does not cover multiple seasonalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(2):  # plot 10 examples\n",
    "    best_model.plot_interpretation(raw_predictions.x, raw_predictions.output, idx=idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 500 paths\n",
    "samples = best_model.loss.sample(raw_predictions.output[\"prediction\"][[0]], n_samples=500)[0]\n",
    "\n",
    "# plot prediction\n",
    "fig = best_model.plot_prediction(raw_predictions.x, raw_predictions.output, idx=0, add_loss_to_title=True)\n",
    "ax = fig.get_axes()[0]\n",
    "# plot first two sampled paths\n",
    "ax.plot(samples[:, 0], color=\"g\", label=\"Sample 1\")\n",
    "ax.plot(samples[:, 1], color=\"r\", label=\"Sample 2\")\n",
    "fig.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the variance of predictions is smaller within each sample than accross all samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Var(all samples) = {samples.var():.4f}\")\n",
    "print(f\"Mean(Var(sample)) = {samples.var(0).mean():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do something new and plot the distribution of sums of forecasts over the entire prediction range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples.sum(0).numpy(), bins=30)\n",
    "plt.xlabel(\"Sum of predictions\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
