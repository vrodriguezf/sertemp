{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Time series forecasting - exercise\n",
    "\n",
    "> Try your best in one of the Monash datasets!\n",
    "\n",
    "Today you'll apply the knowledge acquired in part 5 to perform forecasting on one \n",
    "of the datasets from the Monash time series forecasting archive (TSF). You don't \n",
    "have to build the TSC algorithm from scratch if you don't want to, but rather make\n",
    "use of high level tools. Use the ones used in previous exercises such as:\n",
    "- [aeon](https://github.com/aeon-toolkit/aeon)\n",
    "- [tsai](https://github.com/timeseriesAI/tsai)\n",
    "- [tslearn](https://github.com/tslearn-team/tslearn#available-features)\n",
    "- [sk-time](https://github.com/sktime/sktime)\n",
    "\n",
    "Or new ones seen in this course:\n",
    "- [statsmodels](https://www.statsmodels.org/stable/index.html). Implements traditional\n",
    "statistical forecasting models.\n",
    "- [pytorch-forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/): Pytorch\n",
    "library built on top of [pytorch lightning](https://lightning.ai/docs/pytorch/stable/)\n",
    "that implements several neural forecasting models including NHiTS. \n",
    "\n",
    "We are going to use the sunspot dataset. This dataset contains a single very long \n",
    "daily time series of sunspot numbers from 1818-01-08 to 2020-05-31. Be aware that\n",
    "there is missing data. The nonmissing data version of this dataset was filled with\n",
    "the LOCV method of imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Running the following code is **mandatory**, as it will load the datasets as meant for\n",
    "the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "url_train = 'https://zenodo.org/api/records/4654773/files-archive'\n",
    "url_test = 'https://zenodo.org/api/records/4654722/files-archive'\n",
    "# Download the zip file\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "TEMP_FOLDER = 'raw_data'\n",
    "\n",
    "response = requests.get(url_train)\n",
    "\n",
    "def get(url):\n",
    "    response = requests.get(url)\n",
    "    zip_file_path = os.path.join(TEMP_FOLDER,'temp.zip')  # Specify the path to save the zip file\n",
    "    with open(zip_file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(TEMP_FOLDER)\n",
    "    os.remove(zip_file_path)\n",
    "\n",
    "def extract():\n",
    "    extracted_zipfile = list(os.listdir(TEMP_FOLDER))[0]\n",
    "    extracted_zipfile = os.path.join(TEMP_FOLDER, extracted_zipfile)\n",
    "    with zipfile.ZipFile(extracted_zipfile, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_FOLDER)\n",
    "    os.remove(extracted_zipfile)\n",
    "\n",
    "if not os.path.exists(TEMP_FOLDER):\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir(DATA_FOLDER)\n",
    "\n",
    "get(url_train)\n",
    "extract()\n",
    "get(url_test)\n",
    "extract()\n",
    "\n",
    "os.rmdir(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frequency': 'daily', 'forecast_horizon': None, 'contain_missing_values': True, 'contain_equal_length': True}\n",
      "{'frequency': 'daily', 'forecast_horizon': None, 'contain_missing_values': False, 'contain_equal_length': True}\n"
     ]
    }
   ],
   "source": [
    "# We use the aeon package to load the data. use ``!pip3 install aeon''\n",
    "from aeon.datasets import load_from_tsf_file\n",
    "\n",
    "#DATA_FOLDER = 'data' # Specify if the dataset is alread downloaded\n",
    "TRAIN_DATA_FOLDER = os.path.join(DATA_FOLDER, 'sunspot_dataset_with_missing_values.tsf')\n",
    "TEST_DATA_FOLDER = os.path.join(DATA_FOLDER, 'sunspot_dataset_without_missing_values.tsf')\n",
    "\n",
    "missing_data, missing_metadata = load_from_tsf_file(TRAIN_DATA_FOLDER)\n",
    "nonmissing_data, nonmissing_metadata = load_from_tsf_file(TEST_DATA_FOLDER)\n",
    "\n",
    "print(missing_metadata)\n",
    "print(nonmissing_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sunspot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1818-01-08</th>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818-01-09</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818-01-10</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818-01-11</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818-01-12</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-30</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73924 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sunspot\n",
       "1818-01-08    65.0\n",
       "1818-01-09     NaN\n",
       "1818-01-10     NaN\n",
       "1818-01-11     NaN\n",
       "1818-01-12     NaN\n",
       "...            ...\n",
       "2020-05-27     0.0\n",
       "2020-05-28     0.0\n",
       "2020-05-29     0.0\n",
       "2020-05-30     0.0\n",
       "2020-05-31     2.0\n",
       "\n",
       "[73924 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def to_dataframe(dataset):\n",
    "    numeric_data = np.array(dataset.series_value[0])\n",
    "    interval_date = datetime.timedelta(days=1) * (len(numeric_data) - 1)\n",
    "    start_date = dataset.start_timestamp[0]\n",
    "    date_index = pd.date_range(start_date, interval_date + start_date , freq='D')\n",
    "    return pd.DataFrame(numeric_data, index=date_index, columns=['sunspot'])\n",
    "\n",
    "training_data = to_dataframe(missing_data)[:datetime.datetime(2020, 1, 1)]\n",
    "# The nonmissing data starts from 2020, it si filled with LOCF, some errors are to be expected\n",
    "TESTING_DATA = to_dataframe(nonmissing_data)[datetime.datetime(2020, 1, 1):] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Definition\n",
    "\n",
    "You have available a training and testing dataset. Both ``training_data`` and ``TESTING_DATA`` are not preprocessed, while training data should be preprocessed, the testing data is to never be modified in any way. The training data contains sunspot information up to (not including) 2020-01-1, while the testing data contains information from 2020-01-1 to 2020-05-31. Your task is to adequately forecast the 2020-05-31. You have to generate a forecast for each day of the testing time series. The objective metric to minimize is *RMSE*.\n",
    "\n",
    "> Good luck!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
